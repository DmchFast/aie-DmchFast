# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите): `S07-hw-dataset-01.csv` `S07-hw-dataset-02.csv` `S07-hw-dataset-04.csv`

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (12000 строк, 9 столбцов)
- Признаки: есть числовые (f01-f08) и без пропусков
- Пропуски: нет
- "Подлости" датасета: замечено что признаки имеют разные масштабы по вот такому диапазону (std от 7 до 60),а также вероятны выбросы

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (8000 строк, 4 столбца)
- Признаки: есть числовые (x1, x2, z_noise) и без пропусков
- Пропуски: нет
- "Подлости" датасета: замечено что признаки имеют разные стандартные отклонения по вот такому диапазону(0.66-0.95) - это может повлиять на расстояния в кластеризации


### 1.3 Dataset C

- Файл: `S07-hw-dataset-04.csv`
- Размер: (10000 строк, 33 столбцов)
- Признаки: есть 30 числовых (n01-n30) и 2 категориальных (cat_a, cat_b)
- Пропуски: да, примерно ~2% пропущенных значений в числовых признаках на диапазоне (от 1.7% до 2.1%)
- "Подлости" датасета: замечено что присутствует высокая размерность, категориальные признаки требуют one-hot encoding и есть пропуски в данных


## 2. Protocol

Опишите ваш "честный" unsupervised-протокол.

- Препроцессинг: стандартизация числовых признаков методом StandardScaler, заполнение пропущенных значений: для числовых признаков использовалось среднее значение, для категориальных наиболее частый класс и преобразование категориальных признаков через One-Hot Encoding
- Поиск гиперпараметров:
    - Для K-Means: исследовался диапазон количества кластеров от 2 до 20 с фиксированными параметрами random_state=42 и n_init=10
    - Для DBSCAN: подбирались значения eps [0.5, 1.0, 1.5] и min_samples [3, 5, 10]
    - Выбор оптимальной конфигурации осуществлялся на основе максимизации silhouette score
- Метрики: silhouette / Davies-Bouldin / Calinski-Harabasz для DBSCAN метрики рассчитывались только на точках, не помеченных как шумовые
- Визуализация: PCA(2D) для понижения размерности и визуализации кластеров

## 3. Models

Перечислите, какие модели сравнивали **на каждом датасете**, и какие параметры подбирали.

Минимум (для каждого датасета):

- KMeans (поиск k в диапазоне [2, 20], фиксировали random_state=42, n_init=10)
- DBSCAN (eps в [0.5, 1, 1.5], min_samples в [3, 5, 10], отслеживалась доля шума)

## 4. Results

Для каждого датасета – краткая сводка результатов.

### 4.1 Dataset A

- Лучший метод и параметры: K-Means с количеством кластеров k=2
- Метрики:
  - ilhouette Score: 0.522
  - Davies-Bouldin Index: 0.685
  - Calinski-Harabasz Score: 11786.955
- DBSCAN: показал нулевую долю шумовых точек, DBSCAN также нашел 2 кластера eps=1.5 и min_samples=3, но с худшим silhouette score (0.397), что ниже аналогичного показателя K-Means
- Коротко: K-Means с двумя кластерами демонстрирует более эффективное разделение данных, что подтверждается превосходством по всем ключевым метрикам оценки качества кластеризации

### 4.2 Dataset B

- Лучший метод и параметры: K-Means с двумя кластерами (k=2)
- Метрики:
  - ilhouette Score: 0.307
  - Davies-Bouldin Index: 1.323
  - Calinski-Harabasz Score: 3573.393
- DBSCAN: не смог обнаружить разделяемые кластеры в данных. Вся выборка была отнесена к одному кластеру (доля шума 0%)
- Коротко: K-Means удалось выделить два кластера, качество кластеризации оказалось заметно ниже по сравнению с предыдущим датасетом (о чем свидетельствуют более низкие значения Silhouette и Calinski-Harabasz). DBSCAN показал свою неэффективность для данной структуры данных, не выявив значимого разделения

### 4.3 Dataset C

- Лучший метод и параметры: K-Means с 5 кластерами (k=5)
- Метрики:
  - ilhouette Score: 0.448
  - Davies-Bouldin Index: 0.976
  - Calinski-Harabasz Score: 5103.100
- DBSCAN: не продемонстрировал значимой кластерной структуры. Все наблюдения были отнесены к единому кластеру
- Коротко: KMeans с k=5 показал хорошее качество, что может соответствовать пяти различным группам в данных, которые могут быть связаны с исходными категориальными признаками, исключенными на этапе предобработки

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- K-Means: Эффективно работает при наличии явно выраженных, сферически распределенных кластеров, но демонстрирует высокую чувствительность к масштабу признаков
- DBSCAN: Может оказаться неэффективным на данных с высокой размерностью (проклятие размерности) или при отсутствии четких плотностных группировок
- Масштабирование признаков значительно влияет на результаты, особенно для KMeans

### 5.2 Устойчивость (обязательно для одного датасета)

- Проверка устойчивости: K-Means было выполнено 5 независимых запусков с различными значениями random_state: 0, 10, 20, 30, 40
- Результат: матрица ARI между всеми парами запусков содержит только 1.0, среднее ARI = 1.000
- Вывод: разбиения очень стабильны, что говорит о надежности выбранного алгоритма  K-Means и оптимальности определенных параметров для данной задачи

### 5.3 Интерпретация кластеров

- Интерпретация кластеров проводилась путём анализа усреднённых значений признаков внутри каждого кластера. Для этого использовались данные, преобразованные к исходному масштабу (обратное преобразование от препроцессора).
- Для датасета 01:
  - Выделенные кластеры демонстрируют статистически значимые различия по признакам f02, f04 и f05
  - Различия могут соответствовать различным режимам работы или состояниям, зафиксированным в данных
- Для датасета 04:
  - 5 кластеров с уникальными профилями числовых признаков
  - структура потенциально отражает различные комбинации значений исходных категориальных переменных cat_a и cat_b
- Визуализация методом главных компонент (PCA) в двумерном пространстве подтверждает чёткое разделение кластеров и разделимость согласуется с высокими метриками качества кластеризации
- Вывод: кластеры содержат логически связанные наблюдения с похожими характеристиками, что делает результаты интерпретируемыми и подтверждает осмысленность выполненного разделения данных

## 6. Conclusion

- KMeans:
  - показывает высокую стабильность и воспроизводимость результатов при корректной настройке параметров
- DBSCAN может проявлять нестабильность на данных без выраженной плотностной структуры
- Метрики silhouette, Davies-Bouldin и Calinski-Harabasz дополняют друг друга позволяет получить более полную и объективную картину качества кластеризации
- Препроцессинг данных (масштабирование, обработка пропусков) критически важен для успешной кластеризации
